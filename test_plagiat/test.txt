Humans have always been fascinated by the idea of creating machines that can think and reason like us. This desire stems from a long history of curiosity about the human mind, the way we process information, learn from experiences, and solve complex problems. The dream of replicating human intelligence in a machine has been a driving force behind many scientific and philosophical debates. Early thinkers speculated about the possibility of machines that could not only follow commands but also learn from them, adapt to new information, and make decisions on their own.

As time went on, the idea evolved into the ambition of creating artificial beings with cognitive abilities. Researchers and engineers began to explore ways to develop machines that could process data, make inferences, and improve their performance without relying on rigid instructions. The concept shifted from static, rule-based systems to the idea of machines that could grow smarter through experience, much like the human brain. This led to the emergence of the field of artificial intelligence, which sought to construct systems that could replicate various aspects of human thinking.

At first, many believed that the key to achieving intelligent machines was to understand and recreate the processes occurring inside the human brain. The brain, with its intricate network of neurons, seemed like the perfect model for building machines that could think. This was the foundation for ideas like neural networks, which were inspired by the way biological brains operate—using interconnected neurons to process and transmit information. However, as researchers delved deeper, they realized that replicating the exact structure and complexity of the brain in a machine would be far more difficult than initially imagined.

Instead, they focused on specific cognitive abilities, trying to break down intelligence into smaller, more manageable tasks that could be modeled and simulated with computers. This led to innovations like machine learning and neural networks, which allow systems to learn from data and improve their performance over time without requiring explicit programming for every new task. Though the ultimate goal of creating a truly conscious, thinking machine remains elusive, these developments have significantly advanced our ability to design systems that can perform highly complex tasks—often at a level that exceeds human capability in certain areas.
Artificial neural networks (ANNs), also known as connectionist systems, are computational models inspired by the biological neural networks found in animal brains. These systems are designed to learn and progressively improve their ability to perform tasks by analyzing examples, generally without requiring task-specific programming. For instance, in image recognition, an ANN might learn to distinguish images that contain cats by examining labeled examples, such as "cat" or "no cat," and using the information to identify cats in other images. ANNs are particularly useful for tasks that are difficult to express with traditional computer algorithms using rule-based programming.

Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing "Go").

Deep neural networks
A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.
For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,  and complex DNN have many layers, hence the name "deep" networks. 