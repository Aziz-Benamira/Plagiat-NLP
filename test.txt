Deep learning is a subset of machine learning that uses neural networks with multiple layers to perform tasks like classification, regression, and representation learning. Inspired by biological neuroscience, it stacks artificial neurons into layers and trains them to process data. The term "deep" refers to the use of multiple layers, ranging from three to thousands, in the network. Methods can be supervised, semi-supervised, or unsupervised. Popular architectures include fully connected networks, recurrent neural networks, convolutional neural networks, transformers, generative adversarial networks, and deep belief networks. These architectures have been applied to areas like computer vision, natural language processing, speech recognition, medical imaging, drug design, and climate science, often achieving or surpassing human-level performance. Deep learning models transform input data into increasingly abstract representations through hierarchical layers. For example, in image recognition, layers identify features like edges, shapes, and objects. Unlike traditional machine learning, deep learning automates feature discovery rather than relying on manual feature engineering. The depth of transformations, known as the credit assignment path (CAP), enables deep learning to extract better features with CAP depth greater than two. Deep learning architectures are often built incrementally, layer by layer, and can train on unlabeled data, which is more abundant than labeled data. Theoretical foundations include the universal approximation theorem and probabilistic interpretations, which relate to the model's ability to approximate functions and handle uncertainty in data.