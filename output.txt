
# Détecteur De Plagiat

### Langue du document à tester : anglais
### Titre du document à tester : test.txt
### Résultats de plagiat

- Document : Apprentissage automatique.fr (2.87343% de similitude)
- Document : Apprentissage profond.fr (3.48323% de similitude)
- Document : Deep learning.en (24.8037% de similitude)
- Document : Intelligence artificielle.fr (2.54931% de similitude)
- Document : Machine learning.en (4.53334% de similitude)
- Document : Natural language processing.en (6.69784% de similitude)
- Document : Artificial intelligence.en (5.89301% de similitude)
- Document : B.cpp (0% de similitude)
- Document : Data science.en (2.19007% de similitude)
- Document : A.cpp (0% de similitude)
- Document : C.cpp (0% de similitude)
- Document : Traitement automatique des langues.fr (1.7066% de similitude)



### Ceci est le schéma des couleurs utilisé pour surligner les mots plagiés :
<red>rouge</red>, pour une intensité élevée
<yellow>doré</yellow>, pour une intensité moyenne
<magenta>magenta</magenta>, pour une intensité faible.


### Texte surligné

<red>deep</red> <red>learning</red> is a <magenta>subset</magenta> of <red>machine</red> <red>learning</red> that <red>uses</red> <red>neural</red> <red>networks</red> with <red>multiple</red> <red>layers</red> to perform tasks like <red>classification</red>  <magenta>regression</magenta>  and <red>representation</red> <red>learning</red>  inspired by biological neuroscience  it stacks <magenta>artificial</magenta> <magenta>neurons</magenta> <red>into</red> <red>layers</red> and trains them to process <magenta>data</magenta>  the <red>term</red>  <red>deep</red>  <magenta>refers</magenta> to the <yellow>use</yellow> of <red>multiple</red> <red>layers</red>  <magenta>ranging</magenta> from <magenta>three</magenta> to thousands  in the <red>network</red>  methods <red>can</red> be <magenta>supervised</magenta>  <magenta>semi</magenta> <magenta>supervised</magenta>  or <magenta>unsupervised</magenta>  popular <red>architectures</red> <magenta>include</magenta> <magenta>fully</magenta> <magenta>connected</magenta> <red>networks</red>  <magenta>recurrent</magenta> <red>neural</red> <red>networks</red>  <red>convolutional</red> <red>neural</red> <red>networks</red>  <magenta>transformers</magenta>  generative adversarial <red>networks</red>  and <red>deep</red> <magenta>belief</magenta> <red>networks</red>  these <red>architectures</red> have been applied to areas like computer vision  natural <red>language</red> processing  speech <red>recognition</red>  medical imaging  drug design  and climate science  often achieving or surpassing human level performance  <red>deep</red> <red>learning</red> <red>models</red> <magenta>transform</magenta> <red>input</red> <magenta>data</magenta> <red>into</red> increasingly abstract representations through hierarchical <red>layers</red>  for <red>example</red>  in image <red>recognition</red>  <red>layers</red> identify features like edges  shapes  and objects  unlike traditional <red>machine</red> <red>learning</red>  <red>deep</red> <red>learning</red> automates feature discovery <red>rather</red> <red>than</red> relying on manual feature engineering  the depth of transformations  known <red>as</red> the <magenta>credit</magenta> <magenta>assignment</magenta> <magenta>path</magenta>  <magenta>cap</magenta>   enables <red>deep</red> <red>learning</red> to extract better features with <magenta>cap</magenta> depth greater <red>than</red> two  <red>deep</red> <red>learning</red> <red>architectures</red> are often <red>built</red> incrementally  <red>layer</red> by <red>layer</red>  and <red>can</red> train on unlabeled <magenta>data</magenta>  <red>which</red> is <magenta>more</magenta> <magenta>abundant</magenta> <red>than</red> <magenta>labeled</magenta> <magenta>data</magenta>  theoretical foundations <magenta>include</magenta> the <magenta>universal</magenta> <magenta>approximation</magenta> <magenta>theorem</magenta> and <red>probabilistic</red> interpretations  <red>which</red> relate to the model s ability to approximate functions and handle uncertainty in <magenta>data</magenta> 
<red>neural</red> <red>networks</red> have been <yellow>used</yellow> for <red>implementing</red> <red>language</red> <red>models</red> <red>since</red> the <red>early</red> <red>2000s</red>  <red>lstm</red> <red>helped</red> to <red>improve</red> <red>machine</red> <red>translation</red> and <red>language</red> <red>modeling</red> 
<red>other</red> <red>key</red> <red>techniques</red> in this <red>field</red> are <red>negative</red> <red>sampling</red> and <red>word</red> <red>embedding</red>  <red>word</red> <red>embedding</red>  <red>such</red> <red>as</red> <red>word2vec</red>  <red>can</red> be <red>thought</red> of <red>as</red> a <red>representational</red> <red>layer</red> in a <red>deep</red> <red>learning</red> <red>architecture</red> that <red>transforms</red> an <red>atomic</red> <red>word</red> <red>into</red> a <red>positional</red> <red>representation</red> of the <red>word</red> <red>relative</red> to <red>other</red> <red>words</red> in the <red>dataset</red>  the <red>position</red> is <red>represented</red> <red>as</red> a <red>point</red> in a <red>vector</red> <red>space</red>  <red>using</red> <red>word</red> <red>embedding</red> <red>as</red> an <red>rnn</red> <red>input</red> <red>layer</red> <red>allows</red> the <red>network</red> to <red>parse</red> <red>sentences</red> and <red>phrases</red> <red>using</red> an <red>effective</red> <red>compositional</red> <red>vector</red> <red>grammar</red>  a <red>compositional</red> <red>vector</red> <red>grammar</red> <red>can</red> be <red>thought</red> of <red>as</red> <red>probabilistic</red> <red>context</red> <red>free</red> <red>grammar</red>  <red>pcfg</red>  <red>implemented</red> by an <red>rnn</red>  <red>recursive</red> <red>auto</red> <red>encoders</red> <red>built</red> <red>atop</red> <red>word</red> <red>embeddings</red> <red>can</red> <red>assess</red> <red>sentence</red> <red>similarity</red> and <red>detect</red> <red>paraphrasing</red>  <red>deep</red> <red>neural</red> <red>architectures</red> <red>provide</red> the <red>best</red> <red>results</red> for <red>constituency</red> <red>parsing</red>  <red>sentiment</red> <red>analysis</red>  <red>information</red> <red>retrieval</red>  <red>spoken</red> <red>language</red> <red>understanding</red>  <red>machine</red> <red>translation</red>  <red>contextual</red> <red>entity</red> <red>linking</red>  <red>writing</red> <red>style</red> <red>recognition</red>  <red>named</red> <red>entity</red> <red>recognition</red>  <red>token</red> <red>classification</red>   <red>text</red> <red>classification</red>  and <red>others</red> 
<red>recent</red> <red>developments</red> <red>generalize</red> <red>word</red> <red>embedding</red> to <red>sentence</red> <red>embedding</red> 
<red>google</red> <red>translate</red>  <red>gt</red>  <red>uses</red> a <red>large</red> <red>end</red> to <red>end</red> <red>long</red> <red>short</red> <red>term</red> <red>memory</red>  <red>lstm</red>  <red>network</red>  <red>google</red> <red>neural</red> <red>machine</red> <red>translation</red>  <red>gnmt</red>  <red>uses</red> an <red>example</red> <red>based</red> <red>machine</red> <red>translation</red> <red>method</red> in <red>which</red> the <red>system</red>  <red>learns</red> from <red>millions</red> of <red>examples</red>   it <red>translates</red>  <red>whole</red> <red>sentences</red> at a <red>time</red>  <red>rather</red> <red>than</red> <red>pieces</red>   <red>google</red> <red>translate</red> <red>supports</red> <red>over</red> <red>one</red> <red>hundred</red> <red>languages</red>  the <red>network</red> <red>encodes</red> the  <red>semantics</red> of the <red>sentence</red> <red>rather</red> <red>than</red> <red>simply</red> <red>memorizing</red> <red>phrase</red> to <red>phrase</red> <red>translations</red>   <red>gt</red> <red>uses</red> <red>english</red> <red>as</red> an <red>intermediate</red> <red>between</red> <yellow>most</yellow> <red>language</red> <magenta>pairs</magenta> 
